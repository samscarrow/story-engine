"""
LMStudio Setup Guide and Real LLM Test
Instructions for connecting to real LLMs and testing emotional sequences
"""

import asyncio
import json
from character_simulation_engine_v2 import (
    CharacterState,
    EmotionalState,
    CharacterMemory,
    SimulationEngine,
    LMStudioLLM,
)


async def test_lmstudio_setup():
    """Test LMStudio setup and provide guidance"""

    print("ü§ñ LMSTUDIO SETUP GUIDE & REAL LLM TEST")
    print("=" * 60)
    print()

    print("üìã SETUP CHECKLIST:")
    print("1. ‚úÖ Download and install LMStudio from https://lmstudio.ai/")
    print("2. üß† Load a model (recommended: Qwen2.5-7B-Instruct or similar)")
    print("3. üåê Start the local server (default: localhost:1234)")
    print("4. üîß Enable structured output in server settings")
    print()

    # Test connection
    print("üîç TESTING LMSTUDIO CONNECTION...")
    print("-" * 40)

    lmstudio_llm = LMStudioLLM(endpoint="http://localhost:1234/v1")

    try:
        print("‚è≥ Checking server availability...")
        is_available = await lmstudio_llm.health_check()

        if not is_available:
            print("‚ùå LMStudio server not responding")
            print("üí° TROUBLESHOOTING:")
            print("   ‚Ä¢ Make sure LMStudio is running")
            print(
                "   ‚Ä¢ Check that the server is started (look for green 'Server Running' indicator)"
            )
            print("   ‚Ä¢ Verify the endpoint is localhost:1234")
            print("   ‚Ä¢ Try restarting LMStudio")
            return False

        print("‚úÖ Server is responding!")

        # Test model loading
        print("üß† Testing model availability...")
        try:
            test_response = await lmstudio_llm.generate_response(
                "You are testing a connection. Respond with JSON containing: dialogue, thought, action, emotional_shift.",
                temperature=0.7,
                max_tokens=200,
            )

            print("‚úÖ Model generated a response!")
            print(f"üìè Response length: {len(test_response.content)} characters")

            # Test JSON parsing
            try:
                response_data = json.loads(test_response.content)
                print("‚úÖ JSON parsing successful!")

                required_keys = ["dialogue", "thought", "action", "emotional_shift"]
                missing_keys = [
                    key for key in required_keys if key not in response_data
                ]

                if missing_keys:
                    print(f"‚ö†Ô∏è  Missing required keys: {missing_keys}")
                    print(
                        "üí° Your model might need better prompting or a different model"
                    )
                else:
                    print("‚úÖ All required JSON fields present!")
                    print("üéâ LMStudio setup is PERFECT for character simulation!")
                    return True

            except json.JSONDecodeError as e:
                print(f"‚ùå JSON parsing failed: {e}")
                print("üìù Raw response preview:")
                print(
                    test_response.content[:300] + "..."
                    if len(test_response.content) > 300
                    else test_response.content
                )
                print()
                print("üí° POSSIBLE FIXES:")
                print("   ‚Ä¢ Try a different model (Qwen2.5, Llama-3.1, or similar)")
                print("   ‚Ä¢ Enable structured output in LMStudio server settings")
                print("   ‚Ä¢ Adjust temperature/max_tokens settings")
                return False

        except Exception as e:
            print(f"‚ùå Model generation failed: {e}")
            print("üí° POSSIBLE ISSUES:")
            print("   ‚Ä¢ No model is loaded in LMStudio")
            print("   ‚Ä¢ Model is still loading (check LMStudio interface)")
            print("   ‚Ä¢ Server configuration issue")
            return False

    except Exception as e:
        print(f"‚ùå Connection failed: {e}")
        print("üí° TROUBLESHOOTING:")
        print("   ‚Ä¢ LMStudio might not be installed or running")
        print("   ‚Ä¢ Check if another application is using port 1234")
        print("   ‚Ä¢ Try restarting your computer")
        return False


async def run_real_llm_emotional_test():
    """Run a quick emotional test with real LLM"""

    print("\n" + "=" * 60)
    print("üé≠ REAL LLM EMOTIONAL TEST")
    print("=" * 60)

    # Test LMStudio setup
    setup_success = await test_lmstudio_setup()

    if not setup_success:
        print("\n‚ùå LMStudio setup incomplete. Please fix issues above and try again.")
        print("üìñ For now, you can use the mock experiments:")
        print("   ‚Ä¢ python experiment.py")
        print("   ‚Ä¢ python advanced_experiment.py")
        print("   ‚Ä¢ python dramatic_emotional_journey.py")
        return

    print("\nüéâ RUNNING REAL LLM EMOTIONAL TEST...")
    print("=" * 50)

    # Create LLM and engine
    lmstudio_llm = LMStudioLLM(endpoint="http://localhost:1234/v1")
    engine = SimulationEngine(llm_provider=lmstudio_llm, max_concurrent=1)

    # Create test character
    test_character = CharacterState(
        id="pilate_real_test",
        name="Pontius Pilate",
        backstory={"origin": "Roman equestrian", "career": "Prefect of Judaea"},
        traits=["pragmatic", "conflicted", "duty-bound"],
        values=["Roman law", "order", "survival"],
        fears=["rebellion", "Caesar's wrath"],
        desires=["peace", "clear guidance"],
        emotional_state=EmotionalState(
            anger=0.3, doubt=0.6, fear=0.5, compassion=0.4, confidence=0.6
        ),
        memory=CharacterMemory(),
        current_goal="Navigate this trial successfully",
    )

    # Test scenarios
    test_scenarios = [
        {
            "situation": "Jesus stands before you, accused of claiming to be King of the Jews. What is your first response?",
            "emphasis": "doubt",
            "description": "Initial encounter with doubt emphasis",
        },
        {
            "situation": "The crowd grows violent, demanding crucifixion. You must maintain order while seeking justice.",
            "emphasis": "fear",
            "description": "Crowd pressure with fear emphasis",
        },
        {
            "situation": "Your wife warns you in a message about a dream: 'Have nothing to do with this righteous man.'",
            "emphasis": "compassion",
            "description": "Wife's warning with compassion emphasis",
        },
    ]

    print(f"üé≠ Testing character: {test_character.name}")
    print(
        f"üò∞ Initial emotions: Doubt={test_character.emotional_state.doubt:.1f}, Fear={test_character.emotional_state.fear:.1f}"
    )
    print()

    for i, scenario in enumerate(test_scenarios, 1):
        print(f"üé¨ TEST {i}: {scenario['description']}")
        print(f"üìù Situation: {scenario['situation']}")
        print(f"üéØ Emphasis: {scenario['emphasis']}")

        try:
            print("‚è≥ Generating with real LLM...")

            result = await engine.run_simulation(
                test_character,
                scenario["situation"],
                emphasis=scenario["emphasis"],
                temperature=0.8,
            )

            response = result["response"]

            print(f"üí¨ Pilate: \"{response['dialogue']}\"")
            print(f"ü§î Thought: {response['thought']}")
            print(f"üé≠ Action: {response['action']}")

            # Show emotional changes
            if "emotional_shift" in response and response["emotional_shift"]:
                shifts = []
                for emotion, change in response["emotional_shift"].items():
                    if isinstance(change, (int, float)) and abs(change) > 0.01:
                        direction = "‚ÜóÔ∏è" if change > 0 else "‚ÜòÔ∏è"
                        shifts.append(f"{emotion} {direction} {abs(change):.2f}")

                if shifts:
                    print(f"üìä Emotional shifts: {' | '.join(shifts)}")

            print(
                f"üå°Ô∏è Emotional temperature: {test_character.emotional_state.modulate_temperature():.2f}"
            )
            print()

        except Exception as e:
            print(f"‚ùå Error: {e}")
            print("üí° Try adjusting the model settings or using a different model")
            print()

    print("üéâ REAL LLM TEST COMPLETE!")
    print("=" * 50)
    print(
        "‚úÖ If you see varied, contextual responses above, your setup is working perfectly!"
    )
    print("üöÄ You can now run any experiment with real LLM by editing the scripts:")
    print("   ‚Ä¢ Change MockLLM() to LMStudioLLM() in any experiment file")
    print("   ‚Ä¢ Enjoy realistic, varied character responses!")


async def show_model_recommendations():
    """Show recommended models for character simulation"""

    print("\n" + "=" * 60)
    print("üß† RECOMMENDED MODELS FOR CHARACTER SIMULATION")
    print("=" * 60)
    print()

    recommendations = [
        {
            "model": "Qwen2.5-7B-Instruct",
            "size": "~4GB",
            "pros": [
                "Excellent instruction following",
                "Good JSON output",
                "Fast inference",
            ],
            "cons": ["Medium context window"],
            "rating": "‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê",
        },
        {
            "model": "Llama-3.2-3B-Instruct",
            "size": "~2GB",
            "pros": ["Very fast", "Small memory footprint", "Decent quality"],
            "cons": ["Less creative", "Shorter responses"],
            "rating": "‚≠ê‚≠ê‚≠ê‚≠ê",
        },
        {
            "model": "Mistral-7B-Instruct-v0.3",
            "size": "~4GB",
            "pros": ["Creative responses", "Good character portrayal", "Reliable"],
            "cons": ["Can be verbose", "Moderate speed"],
            "rating": "‚≠ê‚≠ê‚≠ê‚≠ê",
        },
        {
            "model": "Phi-3.5-Mini-Instruct",
            "size": "~2GB",
            "pros": ["Very fast", "Good reasoning", "Compact"],
            "cons": ["Less creative", "Can be repetitive"],
            "rating": "‚≠ê‚≠ê‚≠ê",
        },
    ]

    for rec in recommendations:
        print(f"ü§ñ {rec['model']} {rec['rating']}")
        print(f"   üíæ Size: {rec['size']}")
        print(f"   ‚úÖ Pros: {', '.join(rec['pros'])}")
        print(f"   ‚ö†Ô∏è  Cons: {', '.join(rec['cons'])}")
        print()

    print("üí° TIPS FOR BEST RESULTS:")
    print("   ‚Ä¢ Use temperature 0.7-0.9 for creative character responses")
    print("   ‚Ä¢ Set max_tokens to 300-500 for detailed responses")
    print("   ‚Ä¢ Enable structured output for consistent JSON formatting")
    print("   ‚Ä¢ Try different models to find your preferred style")


if __name__ == "__main__":
    asyncio.run(run_real_llm_emotional_test())
    asyncio.run(show_model_recommendations())
